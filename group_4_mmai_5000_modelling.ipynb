{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "\n",
    "data = pd.read_excel(r'C:\\Users\\anant\\OneDrive\\Desktop\\MBAN\\MBAN 6110\\Datasets\\sfd_data_original.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing target variable 'Victims' into 'Low', 'Medium', 'High'\n",
    "\n",
    "def categorize_victims(victims):\n",
    "    if victims == 0:\n",
    "        return 'Low'\n",
    "    elif victims == 1:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Victims_Cat'] = data['VICTIMS'].apply(categorize_victims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column to represent each shooting incident\n",
    "\n",
    "data['Shootings'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'OCC_DATE' to datetime\n",
    "\n",
    "data['OCC_DATE'] = pd.to_datetime(data['OCC_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by 'OCC_DATE' and 'NEIGHBOURHOOD_158'\n",
    "\n",
    "data.sort_values(by=['NEIGHBOURHOOD_158', 'OCC_DATE'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the periods for the lag and rolling features\n",
    "\n",
    "periods = [30, 60, 90, 180, 365]\n",
    "\n",
    "# Create the lag and rolling features\n",
    "\n",
    "for period in periods:\n",
    "    data[f'Shootings_Last_{period}_Days'] = data.groupby('NEIGHBOURHOOD_158')['Shootings'].transform(lambda x: x.rolling(period).sum())\n",
    "    data[f'Victims_Last_{period}_Days'] = data.groupby('NEIGHBOURHOOD_158')['VICTIMS'].transform(lambda x: x.rolling(period).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outlier Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1, Q3, IQR\n",
    "\n",
    "Q1_victims = data['VICTIMS'].quantile(0.25)\n",
    "Q3_victims = data['VICTIMS'].quantile(0.75)\n",
    "IQR_victims = Q3_victims - Q1_victims\n",
    "\n",
    "# Define outliers and filter them out\n",
    "\n",
    "outliers_victims = (data['VICTIMS'] < (Q1_victims - 3 * IQR_victims)) | (data['VICTIMS'] > (Q3_victims + 3 * IQR_victims))\n",
    "data = data[~outliers_victims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the lag and rolling features with 0\n",
    "\n",
    "lag_and_rolling_cols = [col for col in data.columns if 'Last' in col]\n",
    "data[lag_and_rolling_cols] = data[lag_and_rolling_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'Victims_Cat' to numbers\n",
    "\n",
    "mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "data['Victims_Cat'] = data['Victims_Cat'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "\n",
    "data = pd.get_dummies(data, columns=['OCC_MONTH', 'OCC_DOW', 'OCC_TIME_RANGE', 'DIVISION', 'NEIGHBOURHOOD_158'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input features and the target variable\n",
    "\n",
    "X = data.drop(columns=['OCC_DATE', 'DEATH', 'INJURIES', 'HOOD_158', 'LONG_WGS84', 'LAT_WGS84', 'VICTIMS', 'Shootings', 'Victims_Cat'])\n",
    "y = data['Victims_Cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the date for the start of the last 30 days\n",
    "\n",
    "test_start_date = data['OCC_DATE'].max() - pd.DateOffset(days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training set and test set\n",
    "\n",
    "X_train = X[data['OCC_DATE'] < test_start_date]\n",
    "y_train = y[data['OCC_DATE'] < test_start_date]\n",
    "X_test = X[data['OCC_DATE'] >= test_start_date]\n",
    "y_test = y[data['OCC_DATE'] >= test_start_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First iteration - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anant\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.47368421052631576, 0.19999999999999998, 0.25, 0.22222222222222218)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Random Forest model\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='macro')\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='macro')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
    "\n",
    "accuracy_rf, precision_rf, recall_rf, f1_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to class imbalance, we're creating weights to ensure that this is accounted for when building the trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the scale_pos_weight parameter for each class\n",
    "\n",
    "weights = y_train.value_counts().max() / y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second iteration - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anant\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5789473684210527,\n",
       " 0.3859649122807018,\n",
       " 0.5789473684210527,\n",
       " 0.46315789473684216)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Random Forest model\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train, sample_weight=weights[y_train])\n",
    "\n",
    "# Make predictions on the test set\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "accuracy_rf, precision_rf, recall_rf, f1_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Principal Component Analysis to reduce dimensionality of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to the training set\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Transform the test set\n",
    "\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third iteration - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anant\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6842105263157895,\n",
       " 0.7368421052631579,\n",
       " 0.6842105263157895,\n",
       " 0.5954887218045113)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Random Forest model\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_pca, y_train, sample_weight=weights[y_train])\n",
    "\n",
    "# Make predictions on the test set\n",
    "\n",
    "y_pred_rf = rf.predict(X_test_pca)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "accuracy_rf, precision_rf, recall_rf, f1_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving onto another model, XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anant\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "C:\\Users\\anant\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3157894736842105,\n",
       " 0.20370370370370372,\n",
       " 0.19444444444444445,\n",
       " 0.19595959595959597)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a XGBoost model\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb, average='macro')\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb, average='macro')\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb, average='macro')\n",
    "\n",
    "accuracy_xgb, precision_xgb, recall_xgb, f1_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second iteration - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anant\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5263157894736842,\n",
       " 0.5333333333333333,\n",
       " 0.5833333333333334,\n",
       " 0.5555555555555555)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a XGBoost model with scale_pos_weight\n",
    "\n",
    "xgb_balanced = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_balanced.fit(X_train, y_train, sample_weight=weights[y_train])\n",
    "\n",
    "# Make predictions on the test set\n",
    "\n",
    "y_pred_xgb_balanced = xgb_balanced.predict(X_test)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "\n",
    "accuracy_xgb_balanced = accuracy_score(y_test, y_pred_xgb_balanced)\n",
    "precision_xgb_balanced = precision_score(y_test, y_pred_xgb_balanced, average='macro')\n",
    "recall_xgb_balanced = recall_score(y_test, y_pred_xgb_balanced, average='macro')\n",
    "f1_xgb_balanced = f1_score(y_test, y_pred_xgb_balanced, average='macro')\n",
    "\n",
    "accuracy_xgb_balanced, precision_xgb_balanced, recall_xgb_balanced, f1_xgb_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GridSearchCV for Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'subsample': [0.5, 0.7, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "    'n_estimators' : [100, 200, 500],\n",
    "}\n",
    "\n",
    "# Create a XGBoost classifier\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Create the grid search object\n",
    "scorer = make_scorer(f1_score, average='weighted')\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, scoring=scorer, verbose=True)\n",
    "\n",
    "# Fit the grid search object to the data\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying best parameters and training our XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anant\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7368421052631579\n",
      "Precision: 0.7616099071207431\n",
      "Recall: 0.7368421052631579\n",
      "F1 Score: 0.6805807622504537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anant\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA to the standardized training set\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Transform the test set using the PCA model\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Train an XGBoost model on the PCA-transformed training set\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb.fit(X_train_pca, y_train, sample_weight=weights[y_train])\n",
    "\n",
    "# Make predictions on the PCA-transformed test set\n",
    "y_pred = xgb.predict(X_test_pca)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as an experiment, we wanted to see how our model would perform with only the top 20 neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file again\n",
    "\n",
    "df = pd.read_excel(r'C:\\Users\\anant\\OneDrive\\Desktop\\MBAN\\MBAN 6110\\Datasets\\sfd_data_original.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to datetime datatype\n",
    "\n",
    "df['OCC_DATE'] = pd.to_datetime(df['OCC_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for only top 20 neighbourhoods where shootings have occured\n",
    "\n",
    "top_20_neighbourhoods = df['NEIGHBOURHOOD_158'].value_counts().head(20).index\n",
    "df['NEIGHBOURHOOD_158'] = df['NEIGHBOURHOOD_158'].where(df['NEIGHBOURHOOD_158'].isin(top_20_neighbourhoods), 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anant\\AppData\\Local\\Temp\\ipykernel_18000\\3287101991.py:3: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['thirty_day_period'] = df['OCC_DATE'].dt.to_period('30D')\n"
     ]
    }
   ],
   "source": [
    "# Thirty day period feature\n",
    "\n",
    "df['thirty_day_period'] = df['OCC_DATE'].dt.to_period('30D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back to the original data_grouped dataframe\n",
    "\n",
    "data_grouped = df.groupby(['thirty_day_period', 'NEIGHBOURHOOD_158', 'OCC_TIME_RANGE']).size().reset_index(name='incidents')\n",
    "data_grouped['incidents'] = data_grouped['incidents'].shift(-1)\n",
    "data_grouped = data_grouped[data_grouped['thirty_day_period'] < data_grouped['thirty_day_period'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the less frequent neighbourhoods into an 'Other' category in the aggregated data\n",
    "\n",
    "data_grouped['NEIGHBOURHOOD_158'] = data_grouped['NEIGHBOURHOOD_158'].where(data_grouped['NEIGHBOURHOOD_158'].isin(top_20_neighbourhoods), 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the 'NEIGHBOURHOOD_158' and 'OCC_TIME_RANGE' columns in the aggregated data\n",
    "\n",
    "data_grouped_encoded = pd.get_dummies(data_grouped, columns=['NEIGHBOURHOOD_158', 'OCC_TIME_RANGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the row to split on\n",
    "\n",
    "split_row = int(len(data_grouped_encoded) * 0.8)\n",
    "\n",
    "# Sort the data by 'three_day_period'\n",
    "\n",
    "data_grouped_encoded = data_grouped_encoded.sort_values('thirty_day_period')\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "\n",
    "train = data_grouped_encoded.iloc[:split_row].copy()\n",
    "test = data_grouped_encoded.iloc[split_row:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to transform the 'incidents' column\n",
    "\n",
    "def transform_incidents(incidents):\n",
    "    if incidents <= low_threshold:\n",
    "        return 'Low'\n",
    "    elif incidents <= medium_threshold:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "# Compute the 33rd and 66th percentiles of the 'incidents' column in the training set\n",
    "\n",
    "low_threshold = train['incidents'].quantile(0.33)\n",
    "medium_threshold = train['incidents'].quantile(0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the 'incidents' column in the training set and the test set\n",
    "\n",
    "train['incidents'] = train['incidents'].apply(transform_incidents)\n",
    "test['incidents'] = test['incidents'].apply(transform_incidents)\n",
    "X_train = train.drop(columns=['incidents', 'thirty_day_period'])\n",
    "y_train = train['incidents']\n",
    "X_test = test.drop(columns=['incidents', 'thirty_day_period'])\n",
    "y_test = test['incidents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gradient boosting classifier\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the classifier to predict the classes of the test data\n",
    "\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_xgb = accuracy_score(y_test, y_pred)\n",
    "precision_xgb = precision_score(y_test, y_pred, average='macro')\n",
    "recall_xgb = recall_score(y_test, y_pred, average='macro')\n",
    "f1_xgb = f1_score(y_test, y_pred, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
